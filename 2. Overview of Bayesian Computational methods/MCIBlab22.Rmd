---
title: "BMLab2-2. MC and Intro MCMC"
author: "DataLab CSIC"
date: " "
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

Introducimos aquí conceptos básicos de MCMC. Primero, recordatorio de
MC y comparación con aproximación analítica y  asintótica.
Después nuestro ejemplo introductorio a Gibbs.

## Recordatorio de MC

Consideramos primero el ejemplo de integración Montecarlo 
de clase. Deseamos calcular $I=E(X+X^2)$ cuando $X \sim N(\mu = 1, \sigma =2)$
mediante Montecarlo. Usamos una muestra Montecarlo de tamaño 100.

NB: Empleamos aquí la notación de R para la normal. 
Recuerda que $I=6$.

```{r , eval=FALSE}
# muestra, comprobamos con mean la media
x<-rnorm(100,1,2)
media<-mean(x)
media
# transformación requerida
z<-x+x*x
int<-mean(z)
int
# estimación del error
err<-1/sqrt(100)*sd(z)
err
```
Insistimos en la estocasticidad de estos resultados!!!
Realizando 50 iteraciones del proceso anterior. 
Desplegamos los resultados mediante histogramas. Observa 
la variabilidad.
```{r , eval=FALSE}
int<-matrix(0,1,50)
err<-matrix(0,1,50)
for(i in 1:50)
{
x<-rnorm(100,1,2)
z<-x+x*x
int[i]<-mean(z)
err[i]<-1/sqrt(100)*sd(z)
}
hist(int)
hist(err)
```
Repetimos el ejemplo con mayor tamaño muestral (1000 vs 100). Observa la nueva 
variabilidad.
```{r , eval=FALSE}
int2<-matrix(0,1,50)
err2<-matrix(0,1,50)
for(i in 1:50)
{
x<-rnorm(1000,1,2)
z<-x+x*x
int2[i]<-mean(z)
err2[i]<-1/sqrt(1000)*sd(z)
}
hist(int2)
hist(err2)
```
## Comparación de MC con solución analítica y asintótica 

Comparamos la solución MC con la solución analítica 
y la solución por normalidad asintótica

### Un caso asimétrico

Suponed que descubrimos que la a posteriori es Gamma de 
parámetros $shape=a=6$ y $scale=s=3$. Emplearemos la parametrización 
de R por lo que la densidad es 
$\frac{1}{s^a \Gamma (a)} x^{a-1} \exp {- \frac{x}{s}}, la media 
es $a\times s$ y la varianza $a \times s^2 $. 
Hacemos primero la solución analítica que es
$a\times s$, después por Montecarlo y luego basados en normalidad
asintótica.
```{r, eval=FALSE }
a <- 6
s <- 3
meani<-matrix(0,1,3)
vari<-matrix(0,1,3)
meani[1]<-a*s
vari[1]<-a*(s*s) 
x<-rgamma(1000,shape=a,scale=s)
meani[2]<-mean(x)
vari[2]<-var(x)
mode<-(a-1)*s 
sigma<-sqrt(s*s/(a-1)) 
z<-rnorm(1000,mode,sigma)
meani[3]<-mean(z)
vari[3]<-var(z)
meani
vari
```

### Un caso bimodal

Suponed que descubrimos que la a posteriori es una mixtura
de 2 normales con pesos 0.3 y 0.7, 
medias 3 y 6 y desviaciones típicas 1 y 1,
respectivamente.
Emplearemos la parametrización 
de R. Hacemos primero la solución analítica, después por Montecarlo y luego basados en normalidad
asintótica, suponiendo que el optimizador ha detectado la primera moda y, finalmente,
la segunda moda.
```{r, eval=FALSE }
meani<-matrix(0,1,4)
vari<-matrix(0,1,4)
meani[1]<-0.3*3+0.7* 6
vari[1]<- 0.3* (1+3*3)+ 0.7 *(1+6*6)-mean[1]**2
x<-matrix(0,1,1000)
for (i in 1:1000)
{u<-runif(1)
if (u<0.3) { x[i]<-rnorm(1,3,1)}
    else {x[i] <-rnorm(1,6,1)}
}
meani[2]<-mean(x)
vari[2]<-var(x[1,])
mode1<-3
sigma1<-1
x<-rnorm(1000,mode1,sigma1)
meani[3]<-mean(x)
vari[3]<-var(x)
mode2<-6
sigma2<-1
x<-rnorm(1000,mode2,sigma2)
meani[4]<-mean(x)
vari[4]<-var(x)
meani
vari
```


# Nuestro primer muestreador de Gibbs!!!

Consideramos ahora el ejemplo que empleamos para introducir el muestreador de Gibbs,
el que la conjunta era $\pi (x, y) = \frac{1}{\pi} \exp(-x (1+y)^2)   $.
Queremos calcular $\int \int (x^2 + x*y) \pi (x, y) dx dy$.


```{r, eval=FALSE }
set.seed(1)
x<-matrix(0,1,1000)
y<-matrix(0,1,1000)
x2<-0
for (i in 1:1000)
{x1<-rexp(1, 1+x2*x2)
x[i]<-x1
x2<-rnorm(1,0,1/sqrt(2*x1))
y[i]<-x2}
mean(x)
mean(y)
median(y)
hist(x[1,])
hist(y[1,])
hist(y[1,800:1000])
plot(x[1,1:20],y[1,1:20])
plot(x[1,980:1000],y[1,980:1000])
z<-x[1,]*x[1,]+x[1,]*y[1,]
mean(z)
```

Más cosas pronto.