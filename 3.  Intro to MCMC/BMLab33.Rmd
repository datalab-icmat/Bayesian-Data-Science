---
title: "BMLab3-3. Montecarlo hamiltoniano"
author: "DataLab CSIC"
date: " "
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

En este lab estudiamos en detalle ejemplos de algoritmos de Montecarlo 
hamiltoniano. Primero hacemos un ejemplo directo y luego usamos la 
librería hmclearn, con dos ejemplos, uno de regresión lineal y otro de regresión logística.

## Muestreo de la normal multivariante

Comparamos los tres métodos ya vistos con HMC.

Consideramos muestreo de la normal multivariante mediante HMC.
Primero recordamos (para comparar luego)
el muestreo directo. Cambiamos rho a 0 y mu a (0,0) del ejemplo primigenio para simplificar
la discusión.
```{r, eval=FALSE }
library(MASS)
rho<-0
Sigma<-matrix(c(1,rho,rho,1),2,2)
Sigma
mu<-c(0,0)
mu
z1<-mvrnorm(1000,mu,Sigma)
```
Ahora el muestreador de Gibbs según el esquema ya visto. Quito los comments del lab anterior. En su caso miralos en las versiones anteriores.
```{r, eval=FALSE }
x_update<-function()
{ x1<-rnorm(1,mu[1]+rho*(x2-mu[2]),1-rho*rho)  }
y_update<-function()
{  x2<-rnorm(1,mu[2]+rho*(x1-mu[1]),1-rho*rho) } 
iter<-1500
burnin<-500
set.seed(1)
xg<-matrix(0,2,iter)
x2<-rnorm(1,1,1)
for (i in 1:iter)
{x1<-x_update()
xg[1,i]<-x1
x2<-y_update()
xg[2,i]<-x2}
```
Aquí ponemos el MH con estructura similar. Como distribución generadora de candidatos
empleamos una normal centrada en el estado actual y escalada 1/5 de su tamaño,
esto es $q(\hat {\theta } | \theta )\sim N (\theta , 0.2^2 I)$. Por ser simétrica,
se cancelan en la probabilidad de aceptación los términos correspondientes. 
Quito comentarios de labs anteriores
```{r, eval=FALSE }
iter<-1500
burnin<-500
set.seed(1)
xmh<-matrix(0,2,iter)
xmh[1,1]<-rnorm(1,1,1)
xmh[2,1]<-rnorm(1,1,1)
for(i in 2:iter){
  currentx = xmh[1,i-1]
  currenty=xmh[2,i-1]
  proposedx = rnorm(1,mean=currentx,sd=0.2)
  proposedy = rnorm(1,mean=currenty,sd=0.2)
  A =(dnorm(proposedx,0,1)*dnorm(proposedy,0,1))/(dnorm(currentx,0,1)*dnorm(currenty,0,1)) 
  if(runif(1)<A){
    # aceptamos con probabilidad min (1,A)
    xmh[1,i] = proposedx
    xmh[2,i] = proposedy
  } else {
    # en otro caso rechazamos
    xmh[1,i] = currentx 
    xmh[2,i] = currenty
  }
}
```
Aquí hacemos HMC. Adaptamos primero una función sencilla del gran Radford Neal.

Los argumentos que requiere son

U, función que evalúa menos la log posterior (más una constante)

grad_U, función que evalúa el gradiente de U 

epsilon, paso del leapfrog

L , número de saltos leapfrog

current_q, estado actual 

Los momentos $p$ se muestrean de normales estándar (M es la identidad).
Devuelve el nuevo estado (que será el mismo si se rechaza la 
propuesta realizada después de L saltos leapfrog)

```{r, eval=FALSE }
HMC = function (U, grad_U, epsilon, L, current_q)
{
  q = current_q
  p = rnorm(length(q),0,1)  # normales independientes para generar momentos
  current_p = p
  # Medio paso del momento al principo 
  p = p - epsilon * grad_U(q) / 2
  # L pasos enteros para posición y momento 
  for (i in 1:L)
  {
    # Paso completo para posición 
    q = q + epsilon * p
    # Paso completo para momento, salvo en último paso 
    if (i!=L) p = p - epsilon * grad_U(q)
  }
  # Ultimo medio paso al final para el momento 
  p = p - epsilon * grad_U(q) / 2
  # Negamos momento al final de la trayectoria para hacer simétrica la propuesta
  p = -p
  # Evaluamos energias potencial y cinética al principio y... 
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  # al final de la trayectoria
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  # Decisón de aceptar o rechazar propuesta. exp(...) es probabilidad de aceptar
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return (q)  # acepta
  }
  else
  {
    return (current_q)  # rechaza
  }
}
```
Una vez fija esta función se puede aplicar en cualquier problema. 
Para empezar definimos la
log posterior... 
```{r, eval=FALSE }
U=function (param )
{   u = .5 * (param[1]**2 + param[2]**2)
        return(u) 
}
```
Y su gradiente. OJO aquí parece silly la función, pero así es el gradiente en este caso
dado el mu y el y ...
```{r, eval=FALSE}
grad_U=function ( param  )
{     return (param)}
```
Y ahora ya lo aplicamos. Este trozo sería también ya común (esencialmente).
Definimos los parámetros básicos y estructuras de datos inciales.
```{r, eval=FALSE }
iter<-1500
burnin<-500
epsilon=0.1
L=5
set.seed(1)
xhmc<-matrix(0,2,iter)
current_q<-matrix(0,2)
```
Y ahora ya el hmc, inicializamos y lanzamos.
```{r, eval=FALSE}
xhmc[1,1]<-rnorm(1,1,1)
xhmc[2,1]<-rnorm(1,1,1)
for(i in 2:iter){
  current_q[1] = xhmc[1,i-1]
  current_q[2]=xhmc[2,i-1]
  newq= HMC(U, grad_U, epsilon, L, current_q)
    xhmc[1,i] = newq[1] 
    xhmc[2,i] = newq[2]
  }
```

Finalmente representamos.
```{r, eval=FALSE }
par(mfrow=c(2,2))
plot(z1)
plot(xg[1,burnin:iter],x[2,burnin:iter])
plot(xmh[1,burnin:iter],xmh[2,burnin:iter])
plot(xhmc[1,burnin:iter],xhmc[2,burnin:iter])
# Ojo recordad las acfs!!!!
par(mfrow=c(1,3))
acf(xg[1,burnin:iter])
acf(xmh[1,burnin:iter])
acf(xhmc[1,burnin:iter])
```

## Un ejemplo de regresión 

Los siguientes dos ejemplos se adaptan del artículo de
Thomas y Tu con la librería hmclearn.

Empezamos con un ejemplo de regresión lineal.
Cargamos la librería hmclearn (instalala primero). Usamos los datos
warpbreaks que exploramos brevemente
```{r, eval=FALSE }
library(hmclearn)
help(warpbreaks)
head(warpbreaks)
fix(warpbreaks)
summary(warpbreaks)
plot(warpbreaks)
```
Vamos a predecir el número de breaks a partir de wool y tension incluyendo interacciones.
Hacemos antes el modelo clásico para comparar después. Observa que pone como factores
las variables.
```{r, eval=FALSE }
f <- lm(breaks ~ wool*tension, data = warpbreaks)
summary(f)
```
Definimos las respuestas y la matriz de diseño. Empleamos model.matrix (vemos el help)
```{r, eval=FALSE }
y <- warpbreaks$breaks
y
help(model.matrix)
X <- model.matrix(breaks ~ wool*tension, data=warpbreaks)
X
```
La log-posterior se basa en la elección descrita en clase.
Recordad que poniamos normal-gammma inversa pero haciamos log 
para ponerla en todo R en este último caso. Los parametros 
a, b, sig2beta son a priori y se escogen para tener una a prioir poco
informativa. Separamos entre los pesos de regresión y la gamma (log varianza)
```{r, eval=FALSE }
linear_posterior <- function(theta, y, X, a=1e-4, b=1e-4,
sig2beta=1e3) {
k <- length(theta)
# separa los dos tipos de parametros
beta_param <- as.numeric(theta[1:(k-1)])
gamma_param <- theta[k]
n <- nrow(X)
# hace la log posterior
result <- -(n/2+a)*gamma_param - exp(-gamma_param)/2 *
t(y - X%*%beta_param) %*%
(y - X%*%beta_param) - b*exp(-gamma_param) -
1/2* t(beta_param) %*% beta_param / sig2beta
#hasta aquí
return(result)
}
```
Y  partir de ella sigue el gradiente con los parametros a prori anteriores
```{r, eval=FALSE }
g_linear_posterior <- function(theta, y, X, a=1e-4, b=1e-4,
sig2beta=1e3) {
k <- length(theta)
beta_param <- as.numeric(theta[1:(k-1)])
gamma_param <- theta[k]
n <- nrow(X)
# aqui el grad en beta
grad_beta <- exp(-gamma_param) * t(X) %*%
(y - X%*%beta_param) - beta_param / sig2beta
# aqui el grad en gamma
grad_gamma <- -(n/2 + a) + exp(-gamma_param)/2 *
t(y - X%*%beta_param) %*%
(y - X%*%beta_param) + b*exp(-gamma_param)
# concatenando las componentes
c(as.numeric(grad_beta), as.numeric(grad_gamma))
}
```
Lanzamos ya el hmc. Hacemos antes help(hmc)
```{r, eval=FALSE }
help(hmc)
# Definimos número iteraciones
N <- 2e3
 set.seed(143)
 # Definimos valores de eps para betas y gamma
eps_vals <- c(rep(2e-1, 6), 2e-2)
# Hacemos el call incluidos valores inciales de parámetros
# 20 iteraciones leapfrog
# 2 cadenas en secuencial
# Por defecto M es la identidad
fm1_hmc <- hmc(N, theta.init = c(rep(0, 6), 1),
epsilon = eps_vals, L = 20,
logPOSTERIOR = linear_posterior,
glogPOSTERIOR = g_linear_posterior,
varnames = c(colnames(X), "log_sigma_sq"),
param = list(y = y, X = X), chains = 2,
parallel = FALSE)
```
Desplegamos los resultados.
```{r, eval=FALSE }
summary(fm1_hmc, burnin=200)
plot(fm1_hmc, burnin=200)
```
Finalmente, comparamos con un modelo clásico de regresión lineal. Usamos diagplots de hmclearn
```{r, eval=FALSE }
f <- lm(breaks ~ wool*tension, data = warpbreaks)
# metemos los parámetros clásicos
freq.param <- c(coef(f), 2*log(sigma(f)))
help(diagplots)
diagplots(fm1_hmc, burnin=200, comparison.theta=freq.param)
```

## Regresión logística

Ahora analizamos un modelo de regresión logística con HMC.
Usamos los datos birthweight de la librería MASS. 
```{r, eval=FALSE }
library(MASS)
help(birthwt)
# Otra forma de cargar los datos
birthwt2 <- MASS::birthwt
# Vemos un poco los datos 
dim(birthwt2)
fix(birthwt2)
summary(birthwt2)
# definimos raza como factor
birthwt2$race2 <- factor(birthwt2$race,
labels = c("white", "black", "other"))
# definimos si ha tenido 0 o más prematuros
birthwt2$ptd <- ifelse(birthwt2$ptl > 0, 1, 0)
# definimos si ha tenido 0, 1 o más visitas
birthwt2$ftv2 <- factor(ifelse(birthwt2$ftv > 2, 2, birthwt2$ftv),
labels = c("0", "1", "2+"))
# desplegamos de nuevo los datos para apreciar cambios 
fix(birthwt2)
# Definimos la matriz de diseño. predecimos si va a ser bajo de peso en función de las variables indicadas
X <- model.matrix(low ~ age + lwt + race2 + smoke +
ptd + ht + ui + ftv2,
data = birthwt2)
# definimos la variable de respuesta
y <- birthwt2$low
```
Definimos la log posterior como en clase. Usamos una a priori vaga sobre los betas
```{r, eval=FALSE }
logistic_posterior <- function(theta, y, X, sig2beta=1e3) {
k <- length(theta)
beta_param <- as.numeric(theta)
onev <- rep(1, length(y))
# aqui va la logverosimilitud
ll_bin <- t(beta_param) %*% t(X) %*% (y - 1) -
t(onev) %*% log(1 + exp(-X %*% beta_param))
# aqui la log posterior
result <- ll_bin - 1/2* t(beta_param) %*%
beta_param / sig2beta
return(result)
}
```
y su gradiente
```{r, eval=FALSE }
g_logistic_posterior <- function(theta, y, X, sig2beta=1e3) {
n <- length(y)
k <- length(theta)
beta_param <- as.numeric(theta)
# auqi se define el gradienet
result <- t(X) %*% ( y - 1 + exp(-X %*% beta_param) /
(1 + exp(-X %*% beta_param))) -beta_param/sig2beta
return(result)
}
```
Finalmente inicializamos los parámetros y lanzamos el HMC.
```{r, eval=FALSE }
# 2000 iteraciones
N <- 2e3
# definimos los epsilons de paso en función del parámetor
 continuous_ind <- c(FALSE, TRUE, TRUE, rep(FALSE, 8))
  eps_vals <- ifelse(continuous_ind, 1e-3, 5e-2)

 set.seed(143)
 # llamada a hmc, con valores inicales de parámetros, númeor de saltos, 2 cadenas secuenciales
 fm2_hmc <- hmc(N, theta.init = rep(0, 11),
epsilon = eps_vals, L = 10,
logPOSTERIOR = logistic_posterior,
glogPOSTERIOR = g_logistic_posterior,
param = list(y = y, X = X),
varnames = colnames(X),
chains = 2, parallel = FALSE)
# Vemos contenido del objeto resultante
fm2_hmc
# vemos las tasas de aceptación de las dos cadenas
fm2_hmc$accept/N
```
Presentamos resultados 
```{r, eval=FALSE }
 summary(fm2_hmc, burnin=200)
 plot(fm2_hmc, burnin=200)
```
Comparamos con los estimadores clásicos.
```{r, eval=FALSE }
 f2 <- glm(low ~ age + lwt + race2 + smoke + ptd + ht + ui + ftv2,
data = birthwt2, family = binomial)
 freq.param2 <- coef(f2)
diagplots(fm2_hmc, burnin=200, comparison.theta = freq.param2)
```

Seguimos!!!!



